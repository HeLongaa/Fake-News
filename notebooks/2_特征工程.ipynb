{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e8c177",
   "metadata": {},
   "source": [
    "# 虚假新闻检测 - 特征工程\n",
    "\n",
    "本notebook实现特征工程流程，包括：\n",
    "\n",
    "1. 文本特征\n",
    "   - TF-IDF特征\n",
    "   - Word2Vec词向量\n",
    "   - BERT预训练特征\n",
    "\n",
    "2. 用户特征\n",
    "   - 用户统计特征\n",
    "   - 用户历史行为特征\n",
    "   - 用户信誉度特征\n",
    "\n",
    "3. 传播特征\n",
    "   - 转发量和速度\n",
    "   - 传播深度和广度\n",
    "   - 时间序列特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be494e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # 对于macOS\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d61e93",
   "metadata": {},
   "source": [
    "## 1. 数据加载与预处理\n",
    "\n",
    "首先加载原始数据集，并进行基本的文本预处理：\n",
    "1. 加载谣言和非谣言数据\n",
    "2. 文本清洗（去除URL、@用户名、特殊字符等）\n",
    "3. 中文分词\n",
    "4. 加载用户和传播相关特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据路径\n",
    "DATA_DIR = Path('../data')\n",
    "RUMORS_FILE = DATA_DIR / 'rumors_v170613.json'\n",
    "CED_DATASET = DATA_DIR / 'CED_Dataset'\n",
    "\n",
    "# 加载谣言数据\n",
    "def load_rumors():\n",
    "    rumors = []\n",
    "    with open(RUMORS_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            rumors.append(json.loads(line))\n",
    "    return pd.DataFrame(rumors)\n",
    "\n",
    "# 加载CED数据集\n",
    "def load_ced_dataset():\n",
    "    # 加载原始微博\n",
    "    original_path = CED_DATASET / 'original-microblog'\n",
    "    originals = []\n",
    "    for file in tqdm(list(original_path.glob('*.json')), desc='Loading original posts'):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            originals.append(json.load(f))\n",
    "    \n",
    "    return pd.DataFrame(originals)\n",
    "\n",
    "# 文本预处理\n",
    "def preprocess_text(text):\n",
    "    # 移除URL\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # 移除@用户名\n",
    "    text = re.sub(r'@[\\w\\-]+', '', text)\n",
    "    # 移除特殊字符和表情\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', '', text)\n",
    "    # 移除多余空格\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# 加载停用词\n",
    "def load_stopwords():\n",
    "    stopwords = set(['的', '了', '是', '在', '我', '有', '和', '就', '不', '人', '都', '一', '一个', '上', '也', '很', '到', '说', '要', '去', '你', '会', '着', '没有', '看', '好', '自己', '这'])\n",
    "    return stopwords\n",
    "\n",
    "# 中文分词\n",
    "def tokenize(text, stopwords):\n",
    "    words = jieba.lcut(text)\n",
    "    return [w for w in words if w not in stopwords and len(w) > 1]\n",
    "\n",
    "print(\"加载数据...\")\n",
    "rumors_df = load_rumors()\n",
    "ced_df = load_ced_dataset()\n",
    "\n",
    "print(\"预处理文本...\")\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# 预处理谣言数据\n",
    "rumors_df['clean_text'] = rumors_df['rumorText'].apply(preprocess_text)\n",
    "rumors_df['tokens'] = rumors_df['clean_text'].apply(lambda x: tokenize(x, stopwords))\n",
    "\n",
    "# 预处理CED数据\n",
    "ced_df['clean_text'] = ced_df['text'].apply(preprocess_text)\n",
    "ced_df['tokens'] = ced_df['clean_text'].apply(lambda x: tokenize(x, stopwords))\n",
    "\n",
    "print(f\"谣言数据集大小: {len(rumors_df)}\")\n",
    "print(f\"CED数据集大小: {len(ced_df)}\")\n",
    "\n",
    "# 显示预处理后的样例\n",
    "print(\"\\n预处理后的样例：\")\n",
    "print(rumors_df[['rumorText', 'clean_text', 'tokens']].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d2675",
   "metadata": {},
   "source": [
    "## 2. 文本特征提取\n",
    "\n",
    "我们将实现三种不同的文本特征提取方法：\n",
    "1. TF-IDF特征：捕捉词频-逆文档频率特征\n",
    "2. Word2Vec词向量：学习词的分布式表示\n",
    "3. BERT预训练特征：利用预训练模型提取上下文感知的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accfd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 TF-IDF特征提取\n",
    "print(\"提取TF-IDF特征...\")\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "# 将分词后的文本转换为字符串\n",
    "text_for_tfidf = rumors_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "tfidf_features = tfidf.fit_transform(text_for_tfidf)\n",
    "\n",
    "print(f\"TF-IDF特征维度: {tfidf_features.shape}\")\n",
    "print(\"\\nTop 20 特征词:\")\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names[:20])\n",
    "\n",
    "# 2.2 Word2Vec词向量\n",
    "print(\"\\n训练Word2Vec模型...\")\n",
    "# 准备训练数据\n",
    "sentences = rumors_df['tokens'].tolist() + ced_df['tokens'].tolist()\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 获取文档向量（取所有词向量的平均）\n",
    "def get_document_vector(tokens):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[token])\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "w2v_features = np.array([get_document_vector(tokens) for tokens in rumors_df['tokens']])\n",
    "print(f\"Word2Vec特征维度: {w2v_features.shape}\")\n",
    "\n",
    "# 2.3 BERT特征提取\n",
    "print(\"\\n提取BERT特征...\")\n",
    "# 加载预训练模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "model.eval()\n",
    "\n",
    "def get_bert_features(texts, batch_size=32):\n",
    "    features = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        # 编码文本\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # 使用[CLS]标记的输出作为文档表示\n",
    "            batch_features = outputs[0][:, 0, :].numpy()\n",
    "            features.append(batch_features)\n",
    "    \n",
    "    return np.concatenate(features)\n",
    "\n",
    "# 提取BERT特征（使用一小部分数据作为示例）\n",
    "sample_texts = rumors_df['clean_text'].head(100).tolist()\n",
    "bert_features = get_bert_features(sample_texts)\n",
    "print(f\"BERT特征维度: {bert_features.shape}\")\n",
    "\n",
    "# 可视化特征分布\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# TF-IDF特征分布\n",
    "plt.subplot(131)\n",
    "plt.hist(tfidf_features.data, bins=50)\n",
    "plt.title('TF-IDF特征分布')\n",
    "plt.xlabel('特征值')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# Word2Vec特征分布\n",
    "plt.subplot(132)\n",
    "plt.hist(w2v_features.flatten(), bins=50)\n",
    "plt.title('Word2Vec特征分布')\n",
    "plt.xlabel('特征值')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# BERT特征分布\n",
    "plt.subplot(133)\n",
    "plt.hist(bert_features.flatten(), bins=50)\n",
    "plt.title('BERT特征分布')\n",
    "plt.xlabel('特征值')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbdd4e",
   "metadata": {},
   "source": [
    "## 3. 用户特征提取\n",
    "\n",
    "提取用户相关的特征，包括：\n",
    "1. 基础统计特征：粉丝数、关注数、发文数等\n",
    "2. 账号特征：账号年龄、是否认证等\n",
    "3. 历史行为特征：发文频率、互动率等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 提取用户特征\n",
    "def extract_user_features(user_info):\n",
    "    if not isinstance(user_info, dict):\n",
    "        return {\n",
    "            'followers_count': 0,\n",
    "            'friends_count': 0,\n",
    "            'statuses_count': 0,\n",
    "            'verified': 0,\n",
    "            'account_age_days': 0\n",
    "        }\n",
    "    \n",
    "    # 计算账号年龄（天数）\n",
    "    created_at = pd.to_datetime(user_info.get('created_at', pd.Timestamp.now()))\n",
    "    account_age = (pd.Timestamp.now() - created_at).days\n",
    "    \n",
    "    return {\n",
    "        'followers_count': user_info.get('followers_count', 0),\n",
    "        'friends_count': user_info.get('friends_count', 0),\n",
    "        'statuses_count': user_info.get('statuses_count', 0),\n",
    "        'verified': int(user_info.get('verified', False)),\n",
    "        'account_age_days': account_age\n",
    "    }\n",
    "\n",
    "# 提取用户特征\n",
    "print(\"提取用户特征...\")\n",
    "user_features = ced_df['user'].apply(extract_user_features)\n",
    "user_features_df = pd.DataFrame(user_features.tolist())\n",
    "\n",
    "print(\"\\n用户特征统计：\")\n",
    "print(user_features_df.describe())\n",
    "\n",
    "# 可视化用户特征分布\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 粉丝数分布\n",
    "plt.subplot(221)\n",
    "plt.hist(np.log1p(user_features_df['followers_count']), bins=50)\n",
    "plt.title('粉丝数分布(log)')\n",
    "plt.xlabel('log(粉丝数 + 1)')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 关注数分布\n",
    "plt.subplot(222)\n",
    "plt.hist(np.log1p(user_features_df['friends_count']), bins=50)\n",
    "plt.title('关注数分布(log)')\n",
    "plt.xlabel('log(关注数 + 1)')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 发文数分布\n",
    "plt.subplot(223)\n",
    "plt.hist(np.log1p(user_features_df['statuses_count']), bins=50)\n",
    "plt.title('发文数分布(log)')\n",
    "plt.xlabel('log(发文数 + 1)')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 账号年龄分布\n",
    "plt.subplot(224)\n",
    "plt.hist(user_features_df['account_age_days'], bins=50)\n",
    "plt.title('账号年龄分布（天）')\n",
    "plt.xlabel('账号年龄')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 计算用户特征之间的相关性\n",
    "correlation = user_features_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('用户特征相关性矩阵')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd313f79",
   "metadata": {},
   "source": [
    "## 4. 传播特征提取\n",
    "\n",
    "分析微博的传播特征，包括：\n",
    "1. 基础传播指标：转发数、评论数、点赞数\n",
    "2. 传播速度特征：转发时间间隔分布\n",
    "3. 传播深度特征：转发链条长度\n",
    "4. 传播广度特征：不同用户群的参与度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 提取传播特征\n",
    "def extract_propagation_features(repost_info):\n",
    "    if not isinstance(repost_info, list):\n",
    "        return {\n",
    "            'repost_count': 0,\n",
    "            'avg_interval': 0,\n",
    "            'max_depth': 0,\n",
    "            'unique_users': 0,\n",
    "            'propagation_breadth': 0\n",
    "        }\n",
    "    \n",
    "    # 转发数量\n",
    "    repost_count = len(repost_info)\n",
    "    \n",
    "    # 转发时间间隔\n",
    "    if repost_count > 1:\n",
    "        times = sorted([pd.to_datetime(r['created_at']) for r in repost_info])\n",
    "        intervals = [(t2 - t1).total_seconds() for t1, t2 in zip(times[:-1], times[1:])]\n",
    "        avg_interval = np.mean(intervals)\n",
    "    else:\n",
    "        avg_interval = 0\n",
    "    \n",
    "    # 转发深度\n",
    "    depths = [len(r.get('repost_path', '').split('/')) for r in repost_info]\n",
    "    max_depth = max(depths) if depths else 0\n",
    "    \n",
    "    # 参与用户数\n",
    "    unique_users = len(set(r.get('user_id') for r in repost_info))\n",
    "    \n",
    "    # 传播广度（每层的平均转发数）\n",
    "    depth_counts = pd.Series(depths).value_counts()\n",
    "    propagation_breadth = depth_counts.mean() if not depth_counts.empty else 0\n",
    "    \n",
    "    return {\n",
    "        'repost_count': repost_count,\n",
    "        'avg_interval': avg_interval,\n",
    "        'max_depth': max_depth,\n",
    "        'unique_users': unique_users,\n",
    "        'propagation_breadth': propagation_breadth\n",
    "    }\n",
    "\n",
    "# 提取传播特征\n",
    "print(\"提取传播特征...\")\n",
    "propagation_features = ced_df['reposts'].apply(extract_propagation_features)\n",
    "propagation_features_df = pd.DataFrame(propagation_features.tolist())\n",
    "\n",
    "print(\"\\n传播特征统计：\")\n",
    "print(propagation_features_df.describe())\n",
    "\n",
    "# 可视化传播特征\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 转发数分布\n",
    "plt.subplot(221)\n",
    "plt.hist(np.log1p(propagation_features_df['repost_count']), bins=50)\n",
    "plt.title('转发数分布(log)')\n",
    "plt.xlabel('log(转发数 + 1)')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 平均时间间隔分布\n",
    "plt.subplot(222)\n",
    "plt.hist(np.log1p(propagation_features_df['avg_interval']), bins=50)\n",
    "plt.title('平均转发间隔分布(log)')\n",
    "plt.xlabel('log(平均间隔(秒) + 1)')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 最大深度分布\n",
    "plt.subplot(223)\n",
    "plt.hist(propagation_features_df['max_depth'], bins=50)\n",
    "plt.title('最大转发深度分布')\n",
    "plt.xlabel('最大深度')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 传播广度分布\n",
    "plt.subplot(224)\n",
    "plt.hist(propagation_features_df['propagation_breadth'], bins=50)\n",
    "plt.title('传播广度分布')\n",
    "plt.xlabel('平均每层转发数')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 分析特征与谣言标签的关系\n",
    "ced_df['is_rumor'] = ced_df['label'].fillna(0)\n",
    "propagation_features_df['is_rumor'] = ced_df['is_rumor']\n",
    "\n",
    "# 计算相关性\n",
    "correlation_with_label = propagation_features_df.corr()['is_rumor'].sort_values(ascending=False)\n",
    "print(\"\\n传播特征与谣言标签的相关性：\")\n",
    "print(correlation_with_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c1c45",
   "metadata": {},
   "source": [
    "## 5. 特征整合与保存\n",
    "\n",
    "将所有提取的特征整合到一起：\n",
    "1. 合并文本、用户和传播特征\n",
    "2. 特征标准化\n",
    "3. 保存特征到文件\n",
    "4. 特征重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68baccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 特征整合\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 合并所有特征\n",
    "def combine_features(text_features, user_features, propagation_features):\n",
    "    # 标准化用户特征\n",
    "    user_scaler = StandardScaler()\n",
    "    user_features_scaled = user_scaler.fit_transform(user_features)\n",
    "    \n",
    "    # 标准化传播特征\n",
    "    prop_scaler = StandardScaler()\n",
    "    propagation_features_scaled = prop_scaler.fit_transform(propagation_features)\n",
    "    \n",
    "    # 合并特征\n",
    "    combined_features = np.hstack([\n",
    "        text_features,  # 文本特征（TF-IDF或词向量）\n",
    "        user_features_scaled,  # 用户特征\n",
    "        propagation_features_scaled  # 传播特征\n",
    "    ])\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "# 准备要合并的特征\n",
    "# 这里使用TF-IDF作为文本特征的示例\n",
    "print(\"合并特征...\")\n",
    "combined_features = combine_features(\n",
    "    tfidf_features.toarray(),\n",
    "    user_features_df.drop('account_age_days', axis=1),  # 移除高度相关的特征\n",
    "    propagation_features_df.drop('is_rumor', axis=1)\n",
    ")\n",
    "\n",
    "print(f\"最终特征维度: {combined_features.shape}\")\n",
    "\n",
    "# 保存特征\n",
    "output_dir = Path('../results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(output_dir / 'combined_features.npy', combined_features)\n",
    "np.save(output_dir / 'labels.npy', ced_df['is_rumor'].values)\n",
    "\n",
    "print(\"\\n特征已保存到results目录\")\n",
    "\n",
    "# 特征重要性分析（使用随机森林）\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 训练随机森林模型\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(combined_features, ced_df['is_rumor'])\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_names = (\n",
    "    list(tfidf.get_feature_names_out()) +\n",
    "    list(user_features_df.drop('account_age_days', axis=1).columns) +\n",
    "    list(propagation_features_df.drop('is_rumor', axis=1).columns)\n",
    ")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf.feature_importances_\n",
    "})\n",
    "\n",
    "# 显示最重要的20个特征\n",
    "print(\"\\n最重要的20个特征：\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(20))\n",
    "\n",
    "# 可视化特征重要性\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(20), feature_importance.sort_values('importance', ascending=False)['importance'][:20])\n",
    "plt.xticks(range(20), feature_importance.sort_values('importance', ascending=False)['feature'][:20], rotation=45, ha='right')\n",
    "plt.title('Top 20 最重要特征')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
