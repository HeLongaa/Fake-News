import streamlit as st
import torch
import numpy as np
from pathlib import Path
import sys
import os
import json
import pandas as pd
from io import StringIO

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from src.preprocessing.text_processor import TextProcessor
from src.features.feature_extractor import FeatureExtractor
from src.models.neural_models import BERTClassifier, BiLSTMClassifier
from src.models.traditional_models import SVMClassifier, RandomForestClassifier
from transformers import BertTokenizer
import joblib
from src.utils.data_utils import extract_text_features

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="è™šå‡æ–°é—»æ£€æµ‹ç³»ç»Ÿ",
    page_icon="ğŸ”",
    layout="wide"
)

MODEL_PATHS = {
    'BERT': 'results/models/bert_model.pt',
    'BiLSTM': 'results/models/bilstm_model.pt',
    'SVM': 'results/models/svm_model.joblib',
    'RandomForest': 'results/models/rf_model.joblib'
}

@st.cache_resource
def load_models():
    """åŠ è½½æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹"""
    models = {}
    
    # åŠ è½½ç¥ç»ç½‘ç»œæ¨¡å‹
    if os.path.exists(MODEL_PATHS['BERT']):
        bert_model = BERTClassifier()
        bert_model.load_state_dict(torch.load(MODEL_PATHS['BERT'], map_location=torch.device('cpu')))
        bert_model.eval()
        models['BERT'] = bert_model

    if os.path.exists(MODEL_PATHS['BiLSTM']):
        bilstm_model = BiLSTMClassifier()
        bilstm_model.load_state_dict(torch.load(MODEL_PATHS['BiLSTM'], map_location=torch.device('cpu')))
        bilstm_model.eval()
        models['BiLSTM'] = bilstm_model
    
    # åŠ è½½ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
    if os.path.exists(MODEL_PATHS['SVM']):
        models['SVM'] = joblib.load(MODEL_PATHS['SVM'])
    
    if os.path.exists(MODEL_PATHS['RandomForest']):
        models['RandomForest'] = joblib.load(MODEL_PATHS['RandomForest'])
    
    # åŠ è½½BERT tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    
    # åŠ è½½æ–‡æœ¬å¤„ç†å™¨å’Œç‰¹å¾æå–å™¨
    text_processor = TextProcessor()
    feature_extractor = FeatureExtractor()
    
    return models, tokenizer, text_processor, feature_extractor

def predict_text(text, model_name, models, tokenizer, text_processor, feature_extractor):
    """ä½¿ç”¨é€‰å®šçš„æ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹"""
    # æ–‡æœ¬é¢„å¤„ç†
    cleaned_text = text_processor.clean_text(text)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
    if model_name in ['BERT', 'BiLSTM']:
        # ç¥ç»ç½‘ç»œæ¨¡å‹å¤„ç†
        if model_name == 'BERT':
            # BERTæ¨¡å‹ä½¿ç”¨tokenizer
            encoded = tokenizer.encode_plus(
                cleaned_text,
                add_special_tokens=True,
                max_length=512,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt'
            )
            
            with torch.no_grad():
                outputs = models[model_name](
                    input_ids=encoded['input_ids'],
                    attention_mask=encoded['attention_mask']
                )
        else:
            # BiLSTMæ¨¡å‹å¤„ç†
            text_tensor = feature_extractor.text_to_tensor(cleaned_text)
            with torch.no_grad():
                outputs = models[model_name](text_tensor)
        
        probs = torch.softmax(outputs, dim=1)
        pred_class = torch.argmax(probs, dim=1).item()
        confidence = probs[0][pred_class].item()
    
    else:
        # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹å¤„ç†
        features = feature_extractor.extract_features(cleaned_text)
        features = np.array(features).reshape(1, -1)
        
        pred_class = models[model_name].predict(features)[0]
        proba = models[model_name].predict_proba(features)
        confidence = proba[0][pred_class]
    
    return pred_class, confidence, cleaned_text

def analyze_batch_texts(texts, model_name, models, tokenizer, text_processor, feature_extractor):
    """æ‰¹é‡åˆ†ææ–‡æœ¬"""
    results = []
    for text in texts:
        pred_class, confidence, cleaned_text = predict_text(
            text, model_name, models, tokenizer, text_processor, feature_extractor
        )
        results.append({
            'åŸæ–‡': text,
            'é¢„æµ‹ç»“æœ': 'è™šå‡ä¿¡æ¯' if pred_class == 1 else 'çœŸå®ä¿¡æ¯',
            'ç½®ä¿¡åº¦': f"{confidence*100:.2f}%",
            'ä½¿ç”¨æ¨¡å‹': model_name,
            'å¤„ç†åæ–‡æœ¬': cleaned_text
        })
    return pd.DataFrame(results)

def main():
    # åŠ è½½æ‰€æœ‰æ¨¡å‹å’Œå¤„ç†å™¨
    models, tokenizer, text_processor, feature_extractor = load_models()
    
    if not models:
        st.error("æœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼")
        return
    
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ” è™šå‡æ–°é—»æ£€æµ‹ç³»ç»Ÿ")
    
    # é€‰æ‹©æ¨¡å‹
    available_models = list(models.keys())
    model_name = st.selectbox(
        "é€‰æ‹©æ£€æµ‹æ¨¡å‹",
        available_models,
        help="BERT/BiLSTMï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€‚åˆå¤æ‚æ–‡æœ¬\nSVM/éšæœºæ£®æ—ï¼šä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¤„ç†é€Ÿåº¦æ›´å¿«"
    )
    
    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2 = st.tabs(["å•æ¡æ£€æµ‹", "æ‰¹é‡æ£€æµ‹"])
    
    # ä¾§è¾¹æ 
    st.sidebar.title("å…³äº")
    st.sidebar.info(
        "è¿™æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„è™šå‡æ–°é—»æ£€æµ‹ç³»ç»Ÿã€‚"
        "æ¨¡å‹ä½¿ç”¨å¤§è§„æ¨¡ä¸­æ–‡è°£è¨€æ•°æ®é›†è®­ç»ƒï¼Œ"
        "å¯ä»¥å¸®åŠ©è¯†åˆ«æ½œåœ¨çš„è™šå‡ä¿¡æ¯ã€‚"
    )
    
    st.sidebar.title("ä½¿ç”¨è¯´æ˜")
    st.sidebar.markdown(
        """
        1. åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥å¾…æ£€æµ‹çš„æ–‡æœ¬
        2. ç‚¹å‡»"å¼€å§‹æ£€æµ‹"æŒ‰é’®
        3. ç³»ç»Ÿä¼šæ˜¾ç¤ºæ£€æµ‹ç»“æœå’Œç½®ä¿¡åº¦
        
        æ³¨æ„ï¼š
        - è¾“å…¥æ–‡æœ¬é•¿åº¦ä¸è¦è¶…è¿‡512ä¸ªå­—ç¬¦
        - ç»“æœä»…ä¾›å‚è€ƒï¼Œå»ºè®®ç»“åˆå…¶ä»–ä¿¡æ¯æºéªŒè¯
        """
    )
    
    # å•æ¡æ£€æµ‹é¡µé¢
    with tab1:
        st.markdown("### å•æ¡æ–‡æœ¬æ£€æµ‹")
        text_input = st.text_area(
            "è¯·è¾“å…¥è¦æ£€æµ‹çš„æ–‡æœ¬å†…å®¹ï¼š",
            height=150,
            placeholder="åœ¨æ­¤è¾“å…¥æ–°é—»æ–‡æœ¬..."
        )
        
        col1, col2 = st.columns([4, 1])
        with col1:
            detect_button = st.button("å¼€å§‹æ£€æµ‹", use_container_width=True)
        with col2:
            st.markdown("")  # å ä½
            show_detail = st.checkbox("æ˜¾ç¤ºè¯¦ç»†åˆ†æ")
        
        if detect_button:
            if not text_input:
                st.warning("è¯·è¾“å…¥æ–‡æœ¬å†…å®¹ï¼")
            else:
                with st.spinner("æ­£åœ¨åˆ†æ..."):
                    # è¿›è¡Œé¢„æµ‹
                    pred_class, confidence, cleaned_text = predict_text(
                        text_input, model_name, models, tokenizer, text_processor, feature_extractor
                    )
                
                # æ˜¾ç¤ºç»“æœ
                st.markdown("## æ£€æµ‹ç»“æœ")
                
                # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
                col1, col2 = st.columns(2)
                
                with col1:
                    if pred_class == 1:
                        st.error("âš ï¸ å¯èƒ½æ˜¯è™šå‡ä¿¡æ¯")
                    else:
                        st.success("âœ… å¯èƒ½æ˜¯çœŸå®ä¿¡æ¯")
                
                with col2:
                    st.metric(
                        label="ç½®ä¿¡åº¦",
                        value=f"{confidence*100:.2f}%"
                    )
                
                    # æ˜¾ç¤ºæ–‡æœ¬åˆ†æ
                    if show_detail:
                        st.markdown("### æ–‡æœ¬åˆ†æè¯¦æƒ…")
                        
                        # ä½¿ç”¨expanderæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                        with st.expander("é¢„å¤„ç†åçš„æ–‡æœ¬", expanded=True):
                            st.text(cleaned_text)
                        
                        # æå–å¹¶æ˜¾ç¤ºæ–‡æœ¬ç‰¹å¾
                        features = extract_text_features(cleaned_text)
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            with st.expander("æ–‡æœ¬ç»Ÿè®¡", expanded=True):
                                st.markdown(f"- æ–‡æœ¬é•¿åº¦ï¼š{len(text_input)}")
                                st.markdown(f"- å­—ç¬¦æ•°ï¼š{len(cleaned_text)}")
                                st.markdown(f"- æ ‡ç‚¹ç¬¦å·æ•°ï¼š{features['punctuation_count']}")
                                st.markdown(f"- æƒ…æ„Ÿå€¾å‘ï¼š{features['sentiment_score']:.2f}")
                        
                        with col2:
                            with st.expander("å…³é”®è¯", expanded=True):
                                keywords = text_processor.extract_keywords(cleaned_text)
                                st.write(", ".join(keywords[:10]))
                    
                    # æ˜¾ç¤ºæé†’
                    st.info(
                        "æ³¨æ„ï¼šæ­¤ç»“æœä»…ä¾›å‚è€ƒï¼Œå»ºè®®ï¼š\n"
                        "1. æŸ¥è¯ä¿¡æ¯æ¥æº\n"
                        "2. æ ¸å®å…³é”®äº‹å®\n"
                        "3. å‚è€ƒæƒå¨åª’ä½“æŠ¥é“"
                    )
    
    # æ‰¹é‡æ£€æµ‹é¡µé¢
    with tab2:
        st.markdown("### æ‰¹é‡æ–‡æœ¬æ£€æµ‹")
        st.markdown("æ”¯æŒä¸Šä¼ txtæˆ–csvæ–‡ä»¶è¿›è¡Œæ‰¹é‡æ£€æµ‹ã€‚æ–‡ä»¶æ ¼å¼è¦æ±‚ï¼š")
        st.markdown("- txtæ–‡ä»¶ï¼šæ¯è¡Œä¸€æ¡æ–‡æœ¬")
        st.markdown("- csvæ–‡ä»¶ï¼šåŒ…å«'text'åˆ—")
        
        uploaded_file = st.file_uploader("é€‰æ‹©æ–‡ä»¶", type=['txt', 'csv'])
        
        if uploaded_file is not None:
            try:
                # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                    texts = df['text'].tolist()
                else:
                    content = uploaded_file.getvalue().decode('utf-8')
                    texts = [line.strip() for line in content.split('\\n') if line.strip()]
                
                # æ˜¾ç¤ºå¤„ç†è¿›åº¦
                with st.spinner(f'æ­£åœ¨å¤„ç† {len(texts)} æ¡æ–‡æœ¬...'):
                    results_df = analyze_batch_texts(texts, model_name, models, tokenizer, text_processor, feature_extractor)
                
                # æ˜¾ç¤ºç»“æœ
                st.markdown("### æ£€æµ‹ç»“æœ")
                st.dataframe(results_df, use_container_width=True)
                
                # æä¾›ä¸‹è½½é€‰é¡¹
                csv = results_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    "ä¸‹è½½ç»“æœ",
                    csv,
                    "æ£€æµ‹ç»“æœ.csv",
                    "text/csv",
                    key='download-csv'
                )
            
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
                st.markdown("è¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼æ­£ç¡®ã€‚")

if __name__ == "__main__":
    main()
